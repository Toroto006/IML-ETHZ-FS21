# -*- coding: utf-8 -*-
"""Task2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/Toroto006/iml2021/blob/main/task2/Task2.ipynb

### Trying to solve Task 2 now!

Maybe take a look at [this](https://towardsdatascience.com/machine-learning-with-datetime-feature-engineering-predicting-healthcare-appointment-no-shows-5e4ca3a85f96) as it is a nice way to show how to handle preprocessing.
"""

import pandas as pd
import numpy as np
import sklearn.metrics as metrics
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.preprocessing import StandardScaler
from joblib import Parallel, delayed
import multiprocessing
from sklearn.neural_network import MLPClassifier
import matplotlib as mpl
import matplotlib.pyplot as plt

from sklearn.linear_model import Ridge

from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split

import tensorflow as tf
from tensorflow import keras
import os
import tempfile

trainFeatUrl = 'https://raw.githubusercontent.com/Toroto006/iml2021/main/task2/train_features.csv?token=AH44KNVEVSH3VTA527ZEDNLAP34PQ'
trainLblUrl = 'https://raw.githubusercontent.com/Toroto006/iml2021/main/task2/train_labels.csv?token=AH44KNSBRPXVQOSQM6POTHDAP34PM'

testFeatUrl = 'https://raw.githubusercontent.com/Toroto006/iml2021/main/task2/test_features.csv?token=AH44KNQTTC3NGV7XAHAG73DAP34PU'

trainFeatPD_raw = pd.read_csv(trainFeatUrl)
trainLblPD = pd.read_csv(trainLblUrl)
testFeatPD = pd.read_csv(testFeatUrl)
print("Start downloading for task2")

"""First we notice the difference in times, hence change that"""

np.sort(trainFeatPD_raw["Time"])[-50:]

"""The align_times does exactly that, as we have "the first 12 hours in ICU". This takes for 300000 entries already ~15 on colab though..."""

def align_times(fromDF):
  print(f"len of fromDF {len(fromDF)}/12 = {len(fromDF)/12} i.e. len%12 == 0")
  assert len(fromDF)%12 == 0, "We do not have 12 hours for each patient!"

  fromDF = fromDF.sort_values(["pid", "Time"], ascending=[True, True], inplace=False, ignore_index=True)
  for idx, _ in fromDF.iterrows():
    i = idx % 12 + 1
    fromDF.at[idx, 'Time'] = i
  return fromDF

trainFeatPD_raw = align_times(trainFeatPD_raw)
#trainFeatPD_raw.head(2)

"""We have an incredible amount of NaN, hence we have to do some more preprocessing.  
Trying to follow [Working with Missing Data in Machine Learning](https://towardsdatascience.com/working-with-missing-data-in-machine-learning-9c0a430df4ce)
"""

percentMiss = trainFeatPD_raw.isnull().sum()/len(trainFeatPD_raw)*100
print(f"Amount of NaNs for some per col in %:\n{percentMiss.tail()}")
print(f"That means we have max {np.sort(trainFeatPD_raw.isnull().sum()/len(trainFeatPD_raw)*100)[-1:]}% missing for some...")

def lookAge():
  print(f"Our max age is {trainFeatPD_raw['Age'].max()} and min is {trainFeatPD_raw['Age'].min()}.")
  bins = 10
  print(f"Let's go with {bins} bins for histogramm.")
  trainFeatPD_raw.hist(column='Age', bins=bins)

#lookAge()

"""Looks like we have quite a nice distribution for the age, hence doing some media imputation sounds doable, let's see.  
  
Two options are Simple or KNN. The simple imputer with mean or median does not look good on our data at all, as we have soo much missing data. On the other hand the KNN imputer does not make sense at all as our data is so far apart value wise and hence takes forever to converge, if at all.
"""

def impute(fromDF, constant=True, simple=False, KNN=False):
  if not KNN and not simple and not constant:
    return fromDF
  if KNN:
    imputer = KNNImputer(n_neighbors=2) # Basically never converges
  if simple:
    imputer = SimpleImputer(missing_values=np.nan, strategy='median') # Looks really bad...
  if constant:
    imputer = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)
  return pd.DataFrame(imputer.fit_transform(fromDF), columns=fromDF.columns)

#trainFeatPD = impute(trainFeatPD_raw)
#trainFeatPD

"""Normalizing data necessary/bad/good?"""

def normalize(fromDF, scalar=None, notIn=["pid", "Time"]):
  listNormalize = [lbl for lbl in fromDF.columns if lbl not in notIn]
  normalized = fromDF.copy()
  features = normalized[listNormalize]
  # first create scalar
  if scalar is None:
    scalar = StandardScaler()
    scalar.fit(features.values)
  else:
    print("Taking already trained scalar!")
  # do it
  features = scalar.transform(features.values)
  # TODO is it bad to normalize to test data?? https://scikit-learn.org/stable/modules/neural_networks_supervised.html under 1.17.8 they say so
  normalized[listNormalize] = features
  return pd.DataFrame(normalized, index=fromDF.index, columns=fromDF.columns), scalar

#trainFeatPD_normalized, _ = normalize(trainFeatPD_raw)
#trainFeatPD_normalized.head(5)

"""We came up with a new idea: Do not remove those NaNs in itself, but tell the network which are actually measured and otherwise use the imputed values."""

def timeFeaturize(fromDF):
  timeFeatures = [lbl for lbl in fromDF.columns if lbl not in ["pid", "Time", "Age"]]
  count = fromDF[timeFeatures].copy()
  for feat in timeFeatures:
    count.loc[~count[feat].isnull(), feat] = 1  # not nan
    count.loc[count[feat].isnull(), feat] = 0   # nan
  return count

#timeFeaturized = timeFeaturize(trainFeatPD_normalized)

def timeFeaturizeAll(fromDF):
  features = [lbl for lbl in fromDF.columns if lbl not in ["pid", "Time", "Age"]]
  thisTimerizerNaN = {}
  thisTimerizerFeat = {}
  for idx in range(0,12):
    thisTimerizerNaN[idx] = [f"{idx%12+1}_{lbl}_NNaN" for lbl in features]
    thisTimerizerFeat[idx] = [f"{idx%12+1}_{lbl}" for lbl in features]
  
  def fgrp(df):
    #print(df.shape)
    nanns = timeFeaturize(df)
    person = pd.DataFrame(df["Age"].head(1), columns=["Age"])
    # timerize the exists
    for idx, row in nanns.iterrows():
      person[thisTimerizerNaN[idx%12]] = row.values
    # timerize the actual features
    imputer = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0) # Looks really bad...
    imputedFeatures = pd.DataFrame(imputer.fit_transform(df[features]), columns=features)
    for idx, row in imputedFeatures.iterrows():
      person[thisTimerizerFeat[idx%12]] = row.values
    return person

  # from https://stackoverflow.com/questions/26187759/parallelize-apply-after-pandas-groupby
  def applyParallel(dfGrouped, func):
    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)
    return pd.concat(retLst)

  return applyParallel(fromDF.groupby("pid"), fgrp)

#timeFeaturizeAll(trainFeatPD_normalized.head(12*3)) # TOO SLOW

def timeFeaturizeAllTwo(fromDF, imputeFeatures=True):
  features = [lbl for lbl in fromDF.columns if lbl not in ["pid", "Time", "Age"]]
  thisTimerizerNaN = {}
  thisTimerizerFeat = {}
  for idx in range(0,12):
    thisTimerizerNaN[idx] = [f"{idx%12+1}_{lbl}_NNaN" for lbl in features]
    thisTimerizerFeat[idx] = [f"{idx%12+1}_{lbl}" for lbl in features]
  thisTimerizer2 = ["pid", "Age"]
  thisTimerizerNaN = [y for x in thisTimerizerNaN.items() for y in x[1]]
  thisTimerizerFeat = [y for x in thisTimerizerFeat.items() for y in x[1]]
  # first two done
  timerized = fromDF[fromDF["Time"] == 1][["pid", "Age"]]
  # exists label
  existTimeFeaturized = timeFeaturize(fromDF)
  rows, _ = existTimeFeaturized.shape
  assert rows % 12 == 0, "ROWS not multiple for patients"
  timerizedNaN = pd.DataFrame(existTimeFeaturized.values.reshape(int(rows/12), 34*12), columns=thisTimerizerNaN)
  #print(timerizedNaN)
  # timerized labels
  if imputeFeatures:
    timerizedLabelFeaturized = impute(fromDF, simple=True, constant=False)[features]
  else:
    timerizedLabelFeaturized = fromDF
  rows, _ = timerizedLabelFeaturized.shape
  assert rows % 12 == 0, "ROWS not multiple for patients"
  timerizedFeat = pd.DataFrame(timerizedLabelFeaturized.values.reshape(int(rows/12), 34*12), columns=thisTimerizerFeat)
  #print(timerizedFeat)
  # combine into one
  otherFeat = pd.concat([timerizedNaN, timerizedFeat], axis=1)
  #print(f"shape timerized: {timerized.shape} and shape otherFeat: {otherFeat.shape}")
  timerized[thisTimerizerNaN+thisTimerizerFeat] = otherFeat.values
  return timerized

#timeFeaturizeAllTwo(trainFeatPD_normalized)

"""If we use timeFeaturize as our "golden model" as that one is easier to read and understand and compare it to timeFeaturizeTwo, which uses reshape, we see they are the same for the first few patients. This for us means we do the correct thing."""

VITALS = ['LABEL_RRate', 'LABEL_ABPm', 'LABEL_SpO2', 'LABEL_Heartrate']
TESTS = ['LABEL_BaseExcess', 'LABEL_Fibrinogen', 'LABEL_AST','LABEL_Alkalinephos',
         'LABEL_Bilirubin_total', 'LABEL_Lactate', 'LABEL_TroponinI', 'LABEL_SaO2',
         'LABEL_Bilirubin_direct', 'LABEL_EtCO2']
print(f"We have {len(VITALS)} vital labels, {len(TESTS)} test labels and then Sepsis alone.")

"""Now let's create one preprocessing function, that does all of it to the raw data"""

def preprocess(fromDF, scalar=None, donormalize=True, doimpute=True):
  timeAligned = align_times(fromDF)
  if donormalize:
    normalizedTimeAligned, scalar = normalize(timeAligned, scalar=scalar)
  else:
    normalizedTimeAligned, scalar = timeAligned, None
  timeFeaturized = timeFeaturizeAllTwo(normalizedTimeAligned, imputeFeatures=doimpute)
  return timeFeaturized, scalar

"""Now let's try to do a): ORDERING OF MEDICAL TEST  
Binary classification: 0 means that there will be no further tests of this kind ordered whereas 1 means that at least one is ordered in the remaining stay.

The corresponding columns containing the binary ground truth in train_labels are the TESTS array.

For the scoring function use [sklearn.metric.roc_auc_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score).
"""

def testPreprocess():
  print("Starting preprocessing")
  X, _ = preprocess(trainFeatPD_raw)
  y = trainLblPD.sort_values('pid')
  print(f"Preprocessing done; X.shape: {X.shape} and Y.shape: {y.shape}")

"""First let's try and do MLPClassifier."""

def doMLPClassifier(train_X, train_y):
  print(f"Starting to fit the binary MLPClassifiers!")
  testClf = {}
  i = 0
  for lbl in TESTS:
    clf = MLPClassifier(solver='adam', alpha=1e-5, random_state=42, hidden_layer_sizes=(40, 30, 20, 20))
    clf.fit(train_X.drop("pid", axis='columns'), train_y[lbl])
    testClf[lbl] = clf
    i += 1
    print(f"{lbl} ({i}/{len(TESTS)}) is done!")
  return testClf

def runTestPredictions(testLblClf, test_X):
  print(f"Starting to predict the binary MLPClassifiers!")
  df_submission = {}
  for lbl in TESTS:
    df_submission[lbl] = testLblClf[lbl].predict(test_X.drop("pid", axis='columns'))
  print(f"Done with all of the predictions!")
  return df_submission

#testPreprocess()
#df_submission = runTestPredictions(doMLPClassifier(X, y), X)

def task1Score(actual, submission):
  return np.mean([metrics.roc_auc_score(actual[entry], submission[entry]) for entry in TESTS])

#task1 = task1Score(y, df_submission)
#print(f"Currently task one gives a score of {task1}") # was 0.9987 at some point

"""Uhh wow, just not adding the pid makes the MLPClassifiers work better, now let's see if it is remembering the data or actually learning:"""

# split data and normalize
sorted = trainFeatPD_raw.sort_values(['pid', 'Time'])
rows, cols = sorted.shape
# print(sorted.head(2))
# bring into shape for train_test_split
sameShape = pd.DataFrame(sorted.values.reshape(int(rows/12), 12*37))
# print(sameShape.head(2))
# do the train_test_validation_split
train_data_df, test_data_df = train_test_split(sameShape, test_size=0.2, random_state=43)
train_data_df, val_data_df = train_test_split(train_data_df, test_size=0.2, random_state=43)
# reverse the shaping to before
rows, _ = test_data_df.shape
test_data_df = pd.DataFrame(test_data_df.values.reshape(rows*12, cols), columns=sorted.columns)
rows, _ = train_data_df.shape
train_data_df = pd.DataFrame(train_data_df.values.reshape(rows*12, cols), columns=sorted.columns)
rows, _ = val_data_df.shape
val_data_df = pd.DataFrame(val_data_df.values.reshape(rows*12, cols), columns=sorted.columns)
# preprocess all of them
train_features, scalar = preprocess(train_data_df)
val_features, _ = preprocess(val_data_df, scalar)
test_features, _ = preprocess(test_data_df, scalar)

# split labels
y = trainLblPD.sort_values('pid')
train_labels, test_labels = train_test_split(y, test_size=0.2, random_state=43)
train_labels, val_labels = train_test_split(train_labels, test_size=0.2, random_state=43)
# sort the labels by pid
train_labels = train_labels.sort_values("pid")
val_labels = val_labels.sort_values("pid")
test_labels = test_labels.sort_values("pid")

# to see by eye if it makes sense to now
def test():
  print(train_features.head(1))
  print(train_labels.head(1))
  print(test_features.head(1))
  print(test_labels.head(1))
  print(val_features.head(1))
  print(val_labels.head(1))

#test()

def printShapes():
  print('Training labels shape:', train_labels.shape)
  print('Validation labels shape:', val_labels.shape)
  print('Test labels shape:', test_labels.shape)
  print('Training features shape:', train_features.shape)
  print('Validation features shape:', val_features.shape)
  print('Test features shape:', test_features.shape)

printShapes()

"""Now let's try the MLPClassifiers on our validation set."""

def tryMLPC():
  testClf = doMLPClassifier(train_features, train_labels)
  val_sub = runTestPredictions(testClf, val_features)
  test_sub = runTestPredictions(testClf, test_features)
  print(f"task1 score for val: {task1Score(val_labels, val_sub)}")
  print(f"task1 score for test: {task1Score(test_labels, test_sub)}")

#tryMLPC()

"""As that is barely better than random let's actually try to do a NN.  
For this let's try to follow [imbalanced data](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data) by google.
"""

# dataset imbalance test
for lbl in TESTS:
  neg, pos = np.bincount(y[lbl])
  total = neg + pos
  print(f'Positive: {pos}\t({100 * pos / total}% of total) for {lbl}')
print(f"Total are {total}")

"""Define model and metrics"""

METRICS = [
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
      keras.metrics.Recall(name='recall'),
      keras.metrics.AUC(name='auc'),
      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve
]

def make_model(metrics=METRICS, output_bias=None):
  if output_bias is not None:
    output_bias = tf.keras.initializers.Constant(output_bias)
  model = keras.Sequential([
      keras.layers.Dense(400, activation='relu', input_shape=(train_features.drop(columns=["pid"]).shape[-1],), name="layer1"),
      keras.layers.Dropout(0.25),
      keras.layers.Dense(150, activation='relu', name="layer2"),
      keras.layers.Dropout(0.4),
      keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias),
  ])

  model.compile(
      optimizer=keras.optimizers.Adam(lr=1e-3),
      loss=keras.losses.binary_crossentropy,
      metrics=metrics)

  return model

EPOCHS = 100
BATCH_SIZE = 1024

early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_prc', 
    verbose=1,
    patience=10,
    mode='max',
    restore_best_weights=True)

model = make_model()
model.summary()

"""Test the keras model."""

# to test for single
lbl = TESTS[0]

"""Set a more correct inital bias (not sure we need this as our data is a lot less imbalanced than the one in the example)"""

initial_weights = ""
print(lbl)
neg, pos = np.bincount(y[lbl])
total = neg + pos
initial_bias = np.log([pos/neg])
model = make_model(output_bias=initial_bias)
#model.predict(train_features[:10])
initial_weights = os.path.join(tempfile.mkdtemp(), 'initial_weights_'+lbl)
model.save_weights(initial_weights)

model = make_model()
model.load_weights(initial_weights)
model.layers[-1].bias.assign([0.0])
zero_bias_history = model.fit(
    train_features.drop(columns=["pid"]),
    train_labels[lbl],
    batch_size=BATCH_SIZE,
    epochs=20,
    validation_data=(val_features.drop(columns=["pid"]), val_labels[lbl]), 
    verbose=0)

model = make_model()
model.load_weights(initial_weights)
careful_bias_history = model.fit(
    train_features.drop(columns=["pid"]),
    train_labels[lbl],
    batch_size=BATCH_SIZE,
    epochs=20,
    validation_data=(val_features.drop(columns=["pid"]), val_labels[lbl]), 
    verbose=0)

mpl.rcParams['figure.figsize'] = (12, 10)
colors = plt.rcParams['axes.prop_cycle'].by_key()['color']

def plot_loss(history, label, n):
  # Use a log scale on y-axis to show the wide range of values.
  plt.semilogy(history.epoch, history.history['loss'],
               color=colors[n], label='Train ' + label)
  plt.semilogy(history.epoch, history.history['val_loss'],
               color=colors[n], label='Val ' + label,
               linestyle="--")
  plt.xlabel('Epoch')
  plt.ylabel('Loss')
  plt.legend()

def plot_comparision():
  plot_loss(zero_bias_history, "Zero Bias", 0)
  plot_loss(careful_bias_history, "Careful Bias", 1)

#plot_comparision()

"""Clear advantage not taking any initial weights!  
  
Train the model now
"""

baseline_history = {}
models = {}

def trainTest():
  for lbl in TESTS:
    model = make_model()
    model.layers[-1].bias.assign([0.0])
    baseline_history[lbl] = model.fit(
        train_features.drop(columns=["pid"]),
        train_labels[lbl],
        batch_size=BATCH_SIZE,
        epochs=EPOCHS,
        callbacks=[early_stopping],
        validation_data=(val_features.drop(columns=["pid"]), val_labels[lbl]),
        verbose=0)
    models[lbl] = model
    print(f"Model for {lbl} done!")

trainTest()

train_predictions_baseline = {}
test_predictions_baseline = {}

def predictTest():
  for lbl in TESTS:
    train_predictions_baseline[lbl] = models[lbl].predict(train_features.drop(columns=["pid"]), batch_size=BATCH_SIZE)
    test_predictions_baseline[lbl] = models[lbl].predict(test_features.drop(columns=["pid"]), batch_size=BATCH_SIZE)
    print(f"Prediction done for {lbl}")

predictTest()

def plot_metrics(history, col, lbl):
  metrics = ['loss', 'prc', 'recall', 'precision']
  for n, metric in enumerate(metrics):
    name = metric.replace("_"," ").capitalize()
    plt.subplot(2,2,n+1)
    plt.plot(history.epoch, history.history[metric], color=colors[col], label=f'Train - {lbl}')
    plt.plot(history.epoch, history.history['val_'+metric],
             color=colors[col], linestyle="--", label=f'Val - {lbl}')
    plt.xlabel('Epoch')
    plt.ylabel(name)
    if metric == 'loss':
      plt.ylim([0, plt.ylim()[1]])
    elif metric == 'auc':
      plt.ylim([0.8,1])
    else:
      plt.ylim([0,1])

    plt.legend()

mpl.rcParams['figure.figsize'] = (20, 18)
colors = plt.rcParams['axes.prop_cycle'].by_key()['color']

def plot_roc(name, labels, predictions, **kwargs):
  fp, tp, _ = metrics.roc_curve(labels, predictions)

  plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)
  plt.xlabel('False positives [%]')
  plt.ylabel('True positives [%]')
  #plt.xlim([-0.5,20])
  #plt.ylim([80,100.5])
  plt.grid(True)
  ax = plt.gca()
  ax.set_aspect('equal')

def roc():
  i = 0
  for lbl in TESTS:
    plot_roc(f"Train Baseline - {lbl}", train_labels[lbl], train_predictions_baseline[lbl], color=colors[i])
    plot_roc(f"Test Baseline - {lbl}", test_labels[lbl], test_predictions_baseline[lbl], color=colors[i], linestyle='--')
    i += 1
    plt.legend(loc='lower right')

#roc()

def otherMeasures():
  i = 0
  for lbl in TESTS:
    plot_metrics(baseline_history[lbl], i, lbl)
    i += 1

# otherMeasures

#print(f"For train set I have a task1Score of {task1Score(train_labels, train_predictions_baseline)}") # 0.915 reached with random state 42...
#print(f"For test set I have a task1Score of {task1Score(test_labels, test_predictions_baseline)}") # 0.804 reached with random state 42...

"""Now let's do b), i.e. same binary classification again for sepsis.

Maybe here we could do a test for imbalanced data and actually do the initializiation towards that bias.
"""

lbl = "LABEL_Sepsis"
neg, pos = np.bincount(y[lbl])
total = neg + pos
initial_bias = np.log([pos/neg])
model = make_model(output_bias=initial_bias)
#model.predict(train_features[:10])
initial_weights_sepsis = os.path.join(tempfile.mkdtemp(), 'initial_weights_'+lbl)
model.save_weights(initial_weights_sepsis)

model_sepsis = make_model()
model_sepsis.load_weights(initial_weights_sepsis)
baseline_history_sepsis = model_sepsis.fit(
    train_features.drop(columns=["pid"]),
    train_labels[lbl],
    batch_size=BATCH_SIZE,
    epochs=EPOCHS,
    callbacks=[early_stopping],
    validation_data=(val_features.drop(columns=["pid"]), val_labels[lbl]),
    verbose=0)

def task2Score(actual, submission):
  return metrics.roc_auc_score(actual['LABEL_Sepsis'], submission)

train_predictions_baseline_sepsis = model_sepsis.predict(train_features.drop(columns=["pid"]), batch_size=BATCH_SIZE)
test_predictions_baseline_sepsis = model_sepsis.predict(test_features.drop(columns=["pid"]), batch_size=BATCH_SIZE)
print(f"Prediction done for LABEL_Sepsis")

#print(f"For train set I have a task2Score of {task2Score(train_labels, train_predictions_baseline_sepsis)}") # 0.823 reached with random state 42...
#print(f"For test set I have a task2Score of {task2Score(test_labels, test_predictions_baseline_sepsis)}") # 0.711 reached with random state 42...

"""Let's do now the task c):  
Predict a more general evolution of the patient state, i.e. mean value of a vital sign in the remaining stay. This is a regression task.
The corresponding columns are VITALS. To evaluate the performance of a given model on this sub-task we use [R2-Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html).
"""

clf = {}
for lbl in VITALS:
  clf[lbl] = Ridge(alpha=1.0)
  clf[lbl].fit(train_features.drop(columns=["pid"]), train_labels[lbl])
  print(f"{lbl} clf fit!")

train_predicted = {}
test_predicted = {}
for lbl in VITALS:
  train_predicted[lbl] = clf[lbl].predict(train_features.drop(columns=["pid"]))
  test_predicted[lbl] = clf[lbl].predict(test_features.drop(columns=["pid"]))
  print(f"Prediction of vital {lbl} done")

def task3Score(actual, submission):
  return np.mean([0.5 + 0.5 * np.maximum(0, metrics.r2_score(actual[entry], submission[entry])) for entry in VITALS])

#print(f"For train set I have a task3Score of {task3Score(train_labels, train_predicted)}")
#print(f"For test set I have a task3Score of {task3Score(test_labels, test_predicted)}")

"""The following is modified from score_submission.py"""

def get_score(df_true, df_submission):
    df_submission = df_submission.sort_values('pid')
    df_true = df_true.sort_values('pid')
    task1 = np.mean([metrics.roc_auc_score(df_true[entry], df_submission[entry]) for entry in TESTS])
    task2 = metrics.roc_auc_score(df_true['LABEL_Sepsis'], df_submission['LABEL_Sepsis'])
    task3 = np.mean([0.5 + 0.5 * np.maximum(0, metrics.r2_score(df_true[entry], df_submission[entry])) for entry in VITALS])
    score = np.mean([task1, task2, task3])
    print(f"total score is {score} where the individual scores are {task1}, {task2}, {task3}")
    return score

"""Let's try to combine all together to see the get_score once:"""

train_end = dict(train_predictions_baseline, **train_predicted)
test_end = dict(test_predictions_baseline, **test_predicted)
train_end["LABEL_Sepsis"] = train_predictions_baseline_sepsis 
test_end["LABEL_Sepsis"] = test_predictions_baseline_sepsis

train_res = pd.DataFrame(train_features["pid"].values, columns=["pid"])
for lbl in train_end.keys():
  train_res[lbl] = pd.DataFrame(train_end[lbl], columns=[lbl])
test_res = pd.DataFrame(test_features["pid"].values, columns=["pid"])
for lbl in test_end.keys():
  test_res[lbl] = pd.DataFrame(test_end[lbl], columns=[lbl])

#print(f"Above was train {get_score(train_labels, train_res)}.")
print(f"Above was test {get_score(test_labels, test_res)}.")

"""Looks like this might actually work!  
Let's now train each model one last time with all data and then do the final score try.
"""

X, scalar = preprocess(trainFeatPD_raw)
test_X, _ = preprocess(testFeatPD, scalar=scalar)
y = trainLblPD.sort_values('pid')

"""Do TESTS final"""

# split data and normalize
sorted = trainFeatPD_raw.sort_values(['pid', 'Time'])
rows, cols = sorted.shape
# bring into shape for train_test_split
sameShape = pd.DataFrame(sorted.values.reshape(int(rows/12), 12*37))
# do the train_test_validation_split
train_data_df, val_data_df = train_test_split(sameShape, test_size=0.2, random_state=42)
# reverse the shaping to before
rows, _ = train_data_df.shape
train_data_df = pd.DataFrame(train_data_df.values.reshape(rows*12, cols), columns=sorted.columns)
rows, _ = val_data_df.shape
val_data_df = pd.DataFrame(val_data_df.values.reshape(rows*12, cols), columns=sorted.columns)
# preprocess all of them
train_features, _ = preprocess(train_data_df, scalar)
val_features, _ = preprocess(val_data_df, scalar)
# split labels
y = trainLblPD.sort_values('pid')
train_labels, val_labels = train_test_split(y, test_size=0.2, random_state=42)
# sort the labels by pid
train_labels = train_labels.sort_values("pid")
val_labels = val_labels.sort_values("pid")

baseline_history = {}
models = {}
for lbl in TESTS:
  model = make_model()
  model.layers[-1].bias.assign([0.0])
  baseline_history[lbl] = model.fit(
      train_features.drop(columns=["pid"]),
      train_labels[lbl],
      batch_size=BATCH_SIZE,
      epochs=EPOCHS,
      callbacks=[early_stopping],
      validation_data=(val_features.drop(columns=["pid"]), val_labels[lbl]),
      verbose=0)
  models[lbl] = model
  print(f"Model for {lbl} done!")

tests_predicted = {}
for lbl in TESTS:
  tests_predicted[lbl] = models[lbl].predict(test_X.drop(columns=["pid"]), batch_size=BATCH_SIZE)
  print(f"Prediction done for {lbl}")

"""Do Sepsis final"""

# This makes an initial bias
lbl = "LABEL_Sepsis"
neg, pos = np.bincount(y[lbl])
total = neg + pos
initial_bias = np.log([pos/neg])
model = make_model(output_bias=initial_bias)
#model.predict(train_features[:10])
initial_weights_sepsis = os.path.join(tempfile.mkdtemp(), 'initial_weights_'+lbl)
model.save_weights(initial_weights_sepsis)

model_sepsis = make_model()
model_sepsis.load_weights(initial_weights_sepsis)
baseline_history_sepsis = model_sepsis.fit(
    train_features.drop(columns=["pid"]),
    train_labels[lbl],
    batch_size=BATCH_SIZE,
    epochs=EPOCHS,
    callbacks=[early_stopping],
    validation_data=(val_features.drop(columns=["pid"]), val_labels[lbl]),
    verbose=0)

sepsis_predicted = model_sepsis.predict(test_X.drop(columns=["pid"]), batch_size=BATCH_SIZE)
print(f"Prediction done for LABEL_Sepsis")

"""Do VITALS final"""

clf = {}
for lbl in VITALS:
  clf[lbl] = Ridge(alpha=1.0)
  clf[lbl].fit(X.drop(columns=["pid"]), y[lbl])
  print(f"{lbl} clf fit!")

vitals_predicted = {}
for lbl in VITALS:
  vitals_predicted[lbl] = clf[lbl].predict(test_X.drop(columns=["pid"]))
  print(f"Prediction of vital {lbl} done")

"""Combine them all"""

final_end = dict(tests_predicted, **vitals_predicted)
final_end["LABEL_Sepsis"] = sepsis_predicted

res = pd.DataFrame(test_X["pid"].values, columns=["pid"])
for lbl in final_end.keys():
  res[lbl] = pd.DataFrame(final_end[lbl], columns=[lbl])
#res

"""This is to output the final zip to be downloaded"""

filename = "finalOut.zip"
res.to_csv(filename, index=False, float_format='%.3f', compression='zip')

"""Run the score_submission to test if it works."""

# filename = 'sample.csv'
df_submission = pd.read_csv(filename)

# generate a baseline based on sample.zip
df_true = pd.read_csv(filename)
for label in TESTS + ['LABEL_Sepsis']:
    # round classification labels
    df_true[label] = np.around(df_true[label].values)

get_score(df_true, df_submission)