{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task2.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwCJDlDblbfn"
      },
      "source": [
        "### Trying to solve Task 2 now!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXFcdaT3Ef3w"
      },
      "source": [
        "Maybe take a look at [this](https://towardsdatascience.com/machine-learning-with-datetime-feature-engineering-predicting-healthcare-appointment-no-shows-5e4ca3a85f96) as it is a nice way to show how to handle preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcZGIepbufoi"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from joblib import Parallel, delayed\n",
        "import multiprocessing\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "import tempfile"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oehqk2wZPnlF",
        "outputId": "47323836-3de9-49ea-b5d8-d2142d9f4eaf"
      },
      "source": [
        "trainFeatUrl = 'https://raw.githubusercontent.com/Toroto006/iml2021/main/task2/train_features.csv?token=AH44KNTLRPLZVY6MBFP2QELAREFCU'\n",
        "trainLblUrl = 'https://raw.githubusercontent.com/Toroto006/iml2021/main/task2/train_labels.csv?token=AH44KNTFLY4C253ALILRQP3AREFDA'\n",
        "\n",
        "testFeatUrl = 'https://raw.githubusercontent.com/Toroto006/iml2021/main/task2/test_features.csv?token=AH44KNQXVKJFRL42UMSGLKLAREFCY'\n",
        "\n",
        "trainFeatPD_raw = pd.read_csv(trainFeatUrl)\n",
        "trainLblPD = pd.read_csv(trainLblUrl)\n",
        "testFeatPD = pd.read_csv(testFeatUrl)\n",
        "print(\"Start downloading for task2\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start downloading for task2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DULuvRtfywrb"
      },
      "source": [
        "First we notice the difference in times, hence change that"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4uPDGKfv8Q5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "117916bc-4b62-413d-c103-1b1940f38ada"
      },
      "source": [
        "np.sort(trainFeatPD_raw[\"Time\"])[-50:]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 31,  31,  31,  31,  32,  32,  32,  32,  32,  32,  33,  33,  33,\n",
              "        33,  33,  34,  34,  34,  34,  34,  35,  35,  35,  36,  36,  37,\n",
              "       269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 304,\n",
              "       305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDZPUc-pQgFo"
      },
      "source": [
        "The align_times does exactly that, as we have \"the first 12 hours in ICU\". This takes for 300000 entries already ~15 on colab though..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW2ZIPNzzI1y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d85c323-6752-42db-8d8b-c54caddb8141"
      },
      "source": [
        "def align_times(fromDF):\n",
        "  print(f\"len of fromDF {len(fromDF)}/12 = {len(fromDF)/12} i.e. len%12 == 0\")\n",
        "  assert len(fromDF)%12 == 0, \"We do not have 12 hours for each patient!\"\n",
        "\n",
        "  fromDF = fromDF.sort_values([\"pid\", \"Time\"], ascending=[True, True], inplace=False, ignore_index=True)\n",
        "  for idx, _ in fromDF.iterrows():\n",
        "    i = idx % 12 + 1\n",
        "    fromDF.at[idx, 'Time'] = i\n",
        "  return fromDF\n",
        "\n",
        "trainFeatPD_raw = align_times(trainFeatPD_raw)\n",
        "#trainFeatPD_raw.head(2)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len of fromDF 227940/12 = 18995.0 i.e. len%12 == 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VJ-FjmeR8-C"
      },
      "source": [
        "We have an incredible amount of NaN, hence we have to do some more preprocessing.  \n",
        "Trying to follow [Working with Missing Data in Machine Learning](https://towardsdatascience.com/working-with-missing-data-in-machine-learning-9c0a430df4ce)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USNJI6mWSPYw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b6b09bf-ed50-47f1-b117-0e5f865db640"
      },
      "source": [
        "percentMiss = trainFeatPD_raw.isnull().sum()/len(trainFeatPD_raw)*100\n",
        "print(f\"Amount of NaNs for some per col in %:\\n{percentMiss.tail()}\")\n",
        "print(f\"That means we have max {np.sort(trainFeatPD_raw.isnull().sum()/len(trainFeatPD_raw)*100)[-1:]}% missing for some...\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Amount of NaNs for some per col in %:\n",
            "Heartrate          12.201457\n",
            "Bilirubin_total    97.663420\n",
            "TroponinI          98.343424\n",
            "ABPs               15.920856\n",
            "pH                 89.012021\n",
            "dtype: float64\n",
            "That means we have max [99.68456611]% missing for some...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adV7I8F_iTNr"
      },
      "source": [
        "def lookAge():\n",
        "  print(f\"Our max age is {trainFeatPD_raw['Age'].max()} and min is {trainFeatPD_raw['Age'].min()}.\")\n",
        "  bins = 10\n",
        "  print(f\"Let's go with {bins} bins for histogramm.\")\n",
        "  trainFeatPD_raw.hist(column='Age', bins=bins)\n",
        "\n",
        "#lookAge()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBHI08O_kBYA"
      },
      "source": [
        "Looks like we have quite a nice distribution for the age, hence doing some media imputation sounds doable, let's see.  \n",
        "  \n",
        "Two options are Simple or KNN. The simple imputer with mean or median does not look good on our data at all, as we have soo much missing data. On the other hand the KNN imputer does not make sense at all as our data is so far apart value wise and hence takes forever to converge, if at all."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H6JhYqiiTMV"
      },
      "source": [
        "def impute(fromDF, constant=True, simple=False, KNN=False):\n",
        "  if not KNN and not simple and not constant:\n",
        "    return fromDF\n",
        "  if KNN:\n",
        "    imputer = KNNImputer(n_neighbors=2) # Basically never converges\n",
        "  if simple:\n",
        "    imputer = SimpleImputer(missing_values=np.nan, strategy='median') # Looks really bad...\n",
        "  if constant:\n",
        "    imputer = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
        "  return pd.DataFrame(imputer.fit_transform(fromDF), columns=fromDF.columns)\n",
        "\n",
        "#trainFeatPD = impute(trainFeatPD_raw)\n",
        "#trainFeatPD"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsHtnyT5-3dl"
      },
      "source": [
        "Normalizing data necessary/bad/good?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv9CCmvk-3Pl"
      },
      "source": [
        "def normalize(fromDF, scalar=None, notIn=[\"pid\", \"Time\"]):\n",
        "  listNormalize = [lbl for lbl in fromDF.columns if lbl not in notIn]\n",
        "  normalized = fromDF.copy() \n",
        "  features = normalized[listNormalize]\n",
        "  # first create scalar\n",
        "  if scalar is None:\n",
        "    scalar = StandardScaler()\n",
        "    scalar.fit(features.values)\n",
        "  else:\n",
        "    print(\"Taking already trained scalar!\")\n",
        "  # do it\n",
        "  features = scalar.transform(features.values)\n",
        "  # TODO is it bad to normalize to test data?? https://scikit-learn.org/stable/modules/neural_networks_supervised.html under 1.17.8 they say so\n",
        "  normalized[listNormalize] = features\n",
        "  return pd.DataFrame(normalized, index=fromDF.index, columns=fromDF.columns), scalar\n",
        "\n",
        "#trainFeatPD_normalized, _ = normalize(trainFeatPD_raw)\n",
        "#trainFeatPD_normalized.head(5)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji0As1AZGelk"
      },
      "source": [
        "We came up with a new idea: Do not remove those NaNs in itself, but tell the network which are actually measured and otherwise use the imputed values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbuWLiIUGzvp"
      },
      "source": [
        "def timeFeaturize(fromDF):\n",
        "  timeFeatures = [lbl for lbl in fromDF.columns if lbl not in [\"pid\", \"Time\", \"Age\"]]\n",
        "  count = fromDF[timeFeatures].copy()\n",
        "  for feat in timeFeatures:\n",
        "    count.loc[~count[feat].isnull(), feat] = 1  # not nan\n",
        "    count.loc[count[feat].isnull(), feat] = 0   # nan\n",
        "  return count\n",
        "\n",
        "#timeFeaturized = timeFeaturize(trainFeatPD_normalized)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOXlknlBONh0"
      },
      "source": [
        "def timeFeaturizeAll(fromDF):\n",
        "  features = [lbl for lbl in fromDF.columns if lbl not in [\"pid\", \"Time\", \"Age\"]]\n",
        "  thisTimerizerNaN = {}\n",
        "  thisTimerizerFeat = {}\n",
        "  for idx in range(0,12):\n",
        "    thisTimerizerNaN[idx] = [f\"{idx%12+1}_{lbl}_NNaN\" for lbl in features]\n",
        "    thisTimerizerFeat[idx] = [f\"{idx%12+1}_{lbl}\" for lbl in features]\n",
        "  \n",
        "  def fgrp(df):\n",
        "    #print(df.shape)\n",
        "    nanns = timeFeaturize(df)\n",
        "    person = pd.DataFrame(df[\"Age\"].head(1), columns=[\"Age\"])\n",
        "    # timerize the exists\n",
        "    for idx, row in nanns.iterrows():\n",
        "      person[thisTimerizerNaN[idx%12]] = row.values\n",
        "    # timerize the actual features\n",
        "    imputer = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0) # Looks really bad...\n",
        "    imputedFeatures = pd.DataFrame(imputer.fit_transform(df[features]), columns=features)\n",
        "    for idx, row in imputedFeatures.iterrows():\n",
        "      person[thisTimerizerFeat[idx%12]] = row.values\n",
        "    return person\n",
        "\n",
        "  # from https://stackoverflow.com/questions/26187759/parallelize-apply-after-pandas-groupby\n",
        "  def applyParallel(dfGrouped, func):\n",
        "    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)\n",
        "    return pd.concat(retLst)\n",
        "\n",
        "  return applyParallel(fromDF.groupby(\"pid\"), fgrp)\n",
        "\n",
        "#timeFeaturizeAll(trainFeatPD_normalized.head(12*3)) # TOO SLOW"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdAqGIV6jAJ1"
      },
      "source": [
        "def timeFeaturizeAllTwo(fromDF, imputeFeatures=True):\n",
        "  features = [lbl for lbl in fromDF.columns if lbl not in [\"pid\", \"Time\", \"Age\"]]\n",
        "  thisTimerizerNaN = {}\n",
        "  thisTimerizerFeat = {}\n",
        "  for idx in range(0,12):\n",
        "    thisTimerizerNaN[idx] = [f\"{idx%12+1}_{lbl}_NNaN\" for lbl in features]\n",
        "    thisTimerizerFeat[idx] = [f\"{idx%12+1}_{lbl}\" for lbl in features]\n",
        "  thisTimerizer2 = [\"pid\", \"Age\"]\n",
        "  thisTimerizerNaN = [y for x in thisTimerizerNaN.items() for y in x[1]]\n",
        "  thisTimerizerFeat = [y for x in thisTimerizerFeat.items() for y in x[1]]\n",
        "  # first two done\n",
        "  timerized = fromDF[fromDF[\"Time\"] == 1][[\"pid\", \"Age\"]]\n",
        "  # exists label\n",
        "  existTimeFeaturized = timeFeaturize(fromDF)\n",
        "  rows, _ = existTimeFeaturized.shape\n",
        "  assert rows % 12 == 0, \"ROWS not multiple for patients\"\n",
        "  timerizedNaN = pd.DataFrame(existTimeFeaturized.values.reshape(int(rows/12), 34*12), columns=thisTimerizerNaN)\n",
        "  #print(timerizedNaN)\n",
        "  # timerized labels\n",
        "  if imputeFeatures:\n",
        "    timerizedLabelFeaturized = impute(fromDF, simple=True, constant=False)[features]\n",
        "  else:\n",
        "    timerizedLabelFeaturized = fromDF\n",
        "  rows, _ = timerizedLabelFeaturized.shape\n",
        "  assert rows % 12 == 0, \"ROWS not multiple for patients\"\n",
        "  timerizedFeat = pd.DataFrame(timerizedLabelFeaturized.values.reshape(int(rows/12), 34*12), columns=thisTimerizerFeat)\n",
        "  #print(timerizedFeat)\n",
        "  # combine into one\n",
        "  otherFeat = pd.concat([timerizedNaN, timerizedFeat], axis=1)\n",
        "  #print(f\"shape timerized: {timerized.shape} and shape otherFeat: {otherFeat.shape}\")\n",
        "  timerized[thisTimerizerNaN+thisTimerizerFeat] = otherFeat.values\n",
        "  return timerized\n",
        "\n",
        "#timeFeaturizeAllTwo(trainFeatPD_normalized)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8ZfyMHT5Jb4"
      },
      "source": [
        "If we use timeFeaturize as our \"golden model\" as that one is easier to read and understand and compare it to timeFeaturizeTwo, which uses reshape, we see they are the same for the first few patients. This for us means we do the correct thing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFcRLPh4sTcR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f434d9cf-8699-48f0-a453-275db81ba7f8"
      },
      "source": [
        "VITALS = ['LABEL_RRate', 'LABEL_ABPm', 'LABEL_SpO2', 'LABEL_Heartrate']\n",
        "TESTS = ['LABEL_BaseExcess', 'LABEL_Fibrinogen', 'LABEL_AST','LABEL_Alkalinephos',\n",
        "         'LABEL_Bilirubin_total', 'LABEL_Lactate', 'LABEL_TroponinI', 'LABEL_SaO2',\n",
        "         'LABEL_Bilirubin_direct', 'LABEL_EtCO2']\n",
        "print(f\"We have {len(VITALS)} vital labels, {len(TESTS)} test labels and then Sepsis alone.\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 4 vital labels, 10 test labels and then Sepsis alone.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkMbiwGG6uan"
      },
      "source": [
        "Now let's create one preprocessing function, that does all of it to the raw data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WgCOfm46yKa"
      },
      "source": [
        "def preprocess(fromDF, scalar=None, donormalize=True, doimpute=True):\n",
        "  timeAligned = align_times(fromDF)\n",
        "  if donormalize:\n",
        "    normalizedTimeAligned, scalar = normalize(timeAligned, scalar=scalar)\n",
        "  else:\n",
        "    normalizedTimeAligned, scalar = timeAligned, None\n",
        "  timeFeaturized = timeFeaturizeAllTwo(normalizedTimeAligned, imputeFeatures=doimpute)\n",
        "  return timeFeaturized, scalar"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uGzAiEWQ2a2"
      },
      "source": [
        "Now let's try to do a): ORDERING OF MEDICAL TEST  \n",
        "Binary classification: 0 means that there will be no further tests of this kind ordered whereas 1 means that at least one is ordered in the remaining stay.\n",
        "\n",
        "The corresponding columns containing the binary ground truth in train_labels are the TESTS array.\n",
        "\n",
        "For the scoring function use [sklearn.metric.roc_auc_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWvdyf_eRQ3I"
      },
      "source": [
        "def testPreprocess():\n",
        "  print(\"Starting preprocessing\")\n",
        "  X, _ = preprocess(trainFeatPD_raw)\n",
        "  y = trainLblPD.sort_values('pid')\n",
        "  print(f\"Preprocessing done; X.shape: {X.shape} and Y.shape: {y.shape}\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8tEZh11CVpL"
      },
      "source": [
        "First let's try and do MLPClassifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SxPSS0F9QHP"
      },
      "source": [
        "def doMLPClassifier(train_X, train_y):\n",
        "  print(f\"Starting to fit the binary MLPClassifiers!\")\n",
        "  testClf = {}\n",
        "  i = 0\n",
        "  for lbl in TESTS:\n",
        "    clf = MLPClassifier(solver='adam', alpha=1e-5, random_state=42, hidden_layer_sizes=(40, 30, 20, 20))\n",
        "    clf.fit(train_X.drop(\"pid\", axis='columns'), train_y[lbl])\n",
        "    testClf[lbl] = clf\n",
        "    i += 1\n",
        "    print(f\"{lbl} ({i}/{len(TESTS)}) is done!\")\n",
        "  return testClf\n",
        "\n",
        "def runTestPredictions(testLblClf, test_X):\n",
        "  print(f\"Starting to predict the binary MLPClassifiers!\")\n",
        "  df_submission = {}\n",
        "  for lbl in TESTS:\n",
        "    df_submission[lbl] = testLblClf[lbl].predict(test_X.drop(\"pid\", axis='columns'))\n",
        "  print(f\"Done with all of the predictions!\")\n",
        "  return df_submission\n",
        "\n",
        "#testPreprocess()\n",
        "#df_submission = runTestPredictions(doMLPClassifier(X, y), X)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEDNztSh99m2"
      },
      "source": [
        "def task1Score(actual, submission):\n",
        "  return np.mean([metrics.roc_auc_score(actual[entry], submission[entry]) for entry in TESTS])\n",
        "\n",
        "#task1 = task1Score(y, df_submission)\n",
        "#print(f\"Currently task one gives a score of {task1}\") # was 0.9987 at some point"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1mEQC66biIb"
      },
      "source": [
        "Uhh wow, just not adding the pid makes the MLPClassifiers work better, now let's see if it is remembering the data or actually learning:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08qcyk8xWQWw",
        "outputId": "24b84052-4a4c-4ac8-d30c-89c9d8855efb"
      },
      "source": [
        "random_state = 43\n",
        "# split data and normalize\n",
        "sorted = trainFeatPD_raw.sort_values(['pid', 'Time'])\n",
        "rows, cols = sorted.shape\n",
        "# print(sorted.head(2))\n",
        "# bring into shape for train_test_split\n",
        "sameShape = pd.DataFrame(sorted.values.reshape(int(rows/12), 12*37))\n",
        "# print(sameShape.head(2))\n",
        "# do the train_test_validation_split\n",
        "train_data_df, test_data_df = train_test_split(sameShape, test_size=0.2, random_state=random_state)\n",
        "train_data_df, val_data_df = train_test_split(train_data_df, test_size=0.2, random_state=random_state)\n",
        "# reverse the shaping to before\n",
        "rows, _ = test_data_df.shape\n",
        "test_data_df = pd.DataFrame(test_data_df.values.reshape(rows*12, cols), columns=sorted.columns)\n",
        "rows, _ = train_data_df.shape\n",
        "train_data_df = pd.DataFrame(train_data_df.values.reshape(rows*12, cols), columns=sorted.columns)\n",
        "rows, _ = val_data_df.shape\n",
        "val_data_df = pd.DataFrame(val_data_df.values.reshape(rows*12, cols), columns=sorted.columns)\n",
        "# preprocess all of them\n",
        "train_features, scalar = preprocess(train_data_df)\n",
        "val_features, _ = preprocess(val_data_df, scalar)\n",
        "test_features, _ = preprocess(test_data_df, scalar)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len of fromDF 145872/12 = 12156.0 i.e. len%12 == 0\n",
            "len of fromDF 36480/12 = 3040.0 i.e. len%12 == 0\n",
            "Taking already trained scalar!\n",
            "len of fromDF 45588/12 = 3799.0 i.e. len%12 == 0\n",
            "Taking already trained scalar!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GFe6-TWOeou"
      },
      "source": [
        "# split labels\n",
        "y = trainLblPD.sort_values('pid')\n",
        "train_labels, test_labels = train_test_split(y, test_size=0.2, random_state=random_state)\n",
        "train_labels, val_labels = train_test_split(train_labels, test_size=0.2, random_state=random_state)\n",
        "# sort the labels by pid\n",
        "train_labels = train_labels.sort_values(\"pid\")\n",
        "val_labels = val_labels.sort_values(\"pid\")\n",
        "test_labels = test_labels.sort_values(\"pid\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd_nhjbSkNd4"
      },
      "source": [
        "# to see by eye if it makes sense to now\n",
        "def test():\n",
        "  print(train_features.head(1))\n",
        "  print(train_labels.head(1))\n",
        "  print(test_features.head(1))\n",
        "  print(test_labels.head(1))\n",
        "  print(val_features.head(1))\n",
        "  print(val_labels.head(1))\n",
        "\n",
        "#test()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84HBg45k-owg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdf3e204-a49f-4168-d2b7-d952f1e422c1"
      },
      "source": [
        "def printShapes():\n",
        "  print('Training labels shape:', train_labels.shape)\n",
        "  print('Validation labels shape:', val_labels.shape)\n",
        "  print('Test labels shape:', test_labels.shape)\n",
        "  print('Training features shape:', train_features.shape)\n",
        "  print('Validation features shape:', val_features.shape)\n",
        "  print('Test features shape:', test_features.shape)\n",
        "\n",
        "printShapes()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training labels shape: (12156, 16)\n",
            "Validation labels shape: (3040, 16)\n",
            "Test labels shape: (3799, 16)\n",
            "Training features shape: (12156, 818)\n",
            "Validation features shape: (3040, 818)\n",
            "Test features shape: (3799, 818)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQCZcURDA37w"
      },
      "source": [
        "Now let's try the MLPClassifiers on our validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_SABNZ1A83b"
      },
      "source": [
        "def tryMLPC():\n",
        "  testClf = doMLPClassifier(train_features, train_labels)\n",
        "  val_sub = runTestPredictions(testClf, val_features)\n",
        "  test_sub = runTestPredictions(testClf, test_features)\n",
        "  print(f\"task1 score for val: {task1Score(val_labels, val_sub)}\")\n",
        "  print(f\"task1 score for test: {task1Score(test_labels, test_sub)}\")\n",
        "\n",
        "#tryMLPC()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZLNz4sNCblf"
      },
      "source": [
        "As that is barely better than random let's actually try to do a NN.  \n",
        "For this let's try to follow [imbalanced data](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data) by google."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qB2ANUDUCf2K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a971e7b-4556-4c3f-f4a9-9205ac44ce25"
      },
      "source": [
        "# dataset imbalance test\n",
        "for lbl in TESTS:\n",
        "  neg, pos = np.bincount(y[lbl])\n",
        "  total = neg + pos\n",
        "  print(f'Positive: {pos}\\t({100 * pos / total}% of total) for {lbl}')\n",
        "print(f\"Total are {total}\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive: 5096\t(26.828112661226637% of total) for LABEL_BaseExcess\n",
            "Positive: 1400\t(7.3703606212161095% of total) for LABEL_Fibrinogen\n",
            "Positive: 4554\t(23.974730192155832% of total) for LABEL_AST\n",
            "Positive: 4487\t(23.62200579099763% of total) for LABEL_Alkalinephos\n",
            "Positive: 4570\t(24.05896288496973% of total) for LABEL_Bilirubin_total\n",
            "Positive: 3803\t(20.021058173203475% of total) for LABEL_Lactate\n",
            "Positive: 1895\t(9.976309555146091% of total) for LABEL_TroponinI\n",
            "Positive: 4439\t(23.369307712555937% of total) for LABEL_SaO2\n",
            "Positive: 644\t(3.3903658857594103% of total) for LABEL_Bilirubin_direct\n",
            "Positive: 1254\t(6.601737299289287% of total) for LABEL_EtCO2\n",
            "Total are 18995\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4sv8nBD-3MQ"
      },
      "source": [
        "Define model and metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9tuisEO-5U4"
      },
      "source": [
        "METRICS = [\n",
        "      keras.metrics.TruePositives(name='tp'),\n",
        "      keras.metrics.FalsePositives(name='fp'),\n",
        "      keras.metrics.TrueNegatives(name='tn'),\n",
        "      keras.metrics.FalseNegatives(name='fn'), \n",
        "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      keras.metrics.Precision(name='precision'),\n",
        "      keras.metrics.Recall(name='recall'),\n",
        "      keras.metrics.AUC(name='auc'),\n",
        "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
        "]\n",
        "\n",
        "def make_model(metrics=METRICS, output_bias=None, last_layer=1):\n",
        "  if output_bias is not None:\n",
        "    output_bias = tf.keras.initializers.Constant(output_bias)\n",
        "  model = keras.Sequential([\n",
        "      keras.layers.Dense(400, activation='relu', input_shape=(train_features.drop(columns=[\"pid\"]).shape[-1],), name=\"layer1\"),\n",
        "      keras.layers.Dropout(0.25),\n",
        "      keras.layers.Dense(150, activation='relu', name=\"layer2\"),\n",
        "      keras.layers.Dropout(0.4),\n",
        "      keras.layers.Dense(last_layer, activation='sigmoid', bias_initializer=output_bias),\n",
        "  ])\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(lr=1e-3),\n",
        "      loss=keras.losses.binary_crossentropy,\n",
        "      metrics=metrics)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ny1j1Nv_amM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02de44f9-bead-4b17-d46e-6aaaa3af2b6a"
      },
      "source": [
        "EPOCHS = 100\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_prc', \n",
        "    verbose=1,\n",
        "    patience=10,\n",
        "    mode='max',\n",
        "    restore_best_weights=True)\n",
        "\n",
        "model = make_model()\n",
        "model.summary()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "layer1 (Dense)               (None, 400)               327200    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "layer2 (Dense)               (None, 150)               60150     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 150)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 151       \n",
            "=================================================================\n",
            "Total params: 387,501\n",
            "Trainable params: 387,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNCDkaUoDwwN"
      },
      "source": [
        "Test the keras model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYzhJrF8ThBT"
      },
      "source": [
        "# to test for single\n",
        "lbl = TESTS[0]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XJZylQb_pBb"
      },
      "source": [
        "Set a more correct inital bias (not sure we need this as our data is a lot less imbalanced than the one in the example)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loYW59AO_oIx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58fb06d2-b0b6-4d8c-8b08-4ef12a65fdc4"
      },
      "source": [
        "initial_weights = \"\"\n",
        "print(lbl)\n",
        "neg, pos = np.bincount(y[lbl])\n",
        "total = neg + pos\n",
        "initial_bias = np.log([pos/neg])\n",
        "model = make_model(output_bias=initial_bias)\n",
        "#model.predict(train_features[:10])\n",
        "initial_weights = os.path.join(tempfile.mkdtemp(), 'initial_weights_'+lbl)\n",
        "model.save_weights(initial_weights)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LABEL_BaseExcess\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5sSDV1KmgSC"
      },
      "source": [
        "model = make_model()\n",
        "model.load_weights(initial_weights)\n",
        "model.layers[-1].bias.assign([0.0])\n",
        "zero_bias_history = model.fit(\n",
        "    train_features.drop(columns=[\"pid\"]),\n",
        "    train_labels[lbl],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=20,\n",
        "    validation_data=(val_features.drop(columns=[\"pid\"]), val_labels[lbl]), \n",
        "    verbose=0)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uHJDOpzE2d5"
      },
      "source": [
        "model = make_model()\n",
        "model.load_weights(initial_weights)\n",
        "careful_bias_history = model.fit(\n",
        "    train_features.drop(columns=[\"pid\"]),\n",
        "    train_labels[lbl],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=20,\n",
        "    validation_data=(val_features.drop(columns=[\"pid\"]), val_labels[lbl]), \n",
        "    verbose=0)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QFcFBJgFC0g"
      },
      "source": [
        "mpl.rcParams['figure.figsize'] = (12, 10)\n",
        "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
        "\n",
        "def plot_loss(history, label, n):\n",
        "  # Use a log scale on y-axis to show the wide range of values.\n",
        "  plt.semilogy(history.epoch, history.history['loss'],\n",
        "               color=colors[n], label='Train ' + label)\n",
        "  plt.semilogy(history.epoch, history.history['val_loss'],\n",
        "               color=colors[n], label='Val ' + label,\n",
        "               linestyle=\"--\")\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "\n",
        "def plot_comparision():\n",
        "  plot_loss(zero_bias_history, \"Zero Bias\", 0)\n",
        "  plot_loss(careful_bias_history, \"Careful Bias\", 1)\n",
        "\n",
        "#plot_comparision()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_FQNVbaF83m"
      },
      "source": [
        "Clear advantage not taking any initial weights!  \n",
        "  \n",
        "Train the model now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFYJMEqAGMYK"
      },
      "source": [
        "baseline_history = {}\n",
        "models = {}\n",
        "\n",
        "def trainTest():\n",
        "  for lbl in TESTS:\n",
        "    model = make_model()\n",
        "    model.layers[-1].bias.assign([0.0])\n",
        "    baseline_history[lbl] = model.fit(\n",
        "        train_features.drop(columns=[\"pid\"]),\n",
        "        train_labels[lbl],\n",
        "        batch_size=BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=[early_stopping],\n",
        "        validation_data=(val_features.drop(columns=[\"pid\"]), val_labels[lbl]),\n",
        "        verbose=0)\n",
        "    models[lbl] = model\n",
        "    print(f\"Model for {lbl} done!\")\n",
        "\n",
        "#trainTest()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3gViYOnXNFw"
      },
      "source": [
        "train_predictions_baseline = {}\n",
        "test_predictions_baseline = {}\n",
        "\n",
        "def predictTest():\n",
        "  for lbl in TESTS:\n",
        "    train_predictions_baseline[lbl] = models[lbl].predict(train_features.drop(columns=[\"pid\"]), batch_size=BATCH_SIZE)\n",
        "    test_predictions_baseline[lbl] = models[lbl].predict(test_features.drop(columns=[\"pid\"]), batch_size=BATCH_SIZE)\n",
        "    print(f\"Prediction done for {lbl}\")\n",
        "\n",
        "#predictTest()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkm61W1Ljbga"
      },
      "source": [
        "test all lbls in one"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g48rYEpjdZu",
        "outputId": "5a3ab73c-7ec2-47f6-f832-32a18a8f0013"
      },
      "source": [
        "theseTESTS = TESTS + [\"LABEL_Sepsis\"]\n",
        "def allTESTS(train_features, train_labels, val_features, val_labels):\n",
        "  neg, pos = np.bincount(y[\"LABEL_Sepsis\"])\n",
        "  total = neg + pos\n",
        "  initial_bias = np.log([pos/neg])\n",
        "  model = make_model(last_layer=11)\n",
        "  biases = np.concatenate(([0.0 for _ in range(0,10)], initial_bias))\n",
        "  model.layers[-1].bias.assign(biases)\n",
        "  baseline_history = model.fit(\n",
        "      train_features.drop(columns=[\"pid\"]),\n",
        "      train_labels[theseTESTS],\n",
        "      batch_size=BATCH_SIZE,\n",
        "      epochs=EPOCHS,\n",
        "      callbacks=[early_stopping],\n",
        "      validation_data=(val_features.drop(columns=[\"pid\"]), val_labels[theseTESTS]),\n",
        "      verbose=0)\n",
        "  return model\n",
        "\n",
        "model = allTESTS(train_features, train_labels, val_features, val_labels)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00025: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbP4ek4vkAOY"
      },
      "source": [
        "def doAllTests(model, X_features):\n",
        "  train_predictions_baseline = model.predict(X_features.drop(columns=[\"pid\"]), batch_size=BATCH_SIZE)\n",
        "  train_predictions_baseline = pd.DataFrame(train_predictions_baseline, columns=theseTESTS)\n",
        "  return train_predictions_baseline\n",
        "\n",
        "train_predictions_baseline = doAllTests(model, train_features)\n",
        "test_predictions_baseline = doAllTests(model, test_features)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNd8jAC8GgC5"
      },
      "source": [
        "def plot_metrics(history, col, lbl):\n",
        "  metrics = ['loss', 'prc', 'recall', 'precision']\n",
        "  for n, metric in enumerate(metrics):\n",
        "    name = metric.replace(\"_\",\" \").capitalize()\n",
        "    plt.subplot(2,2,n+1)\n",
        "    plt.plot(history.epoch, history.history[metric], color=colors[col], label=f'Train - {lbl}')\n",
        "    plt.plot(history.epoch, history.history['val_'+metric],\n",
        "             color=colors[col], linestyle=\"--\", label=f'Val - {lbl}')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel(name)\n",
        "    if metric == 'loss':\n",
        "      plt.ylim([0, plt.ylim()[1]])\n",
        "    elif metric == 'auc':\n",
        "      plt.ylim([0.8,1])\n",
        "    else:\n",
        "      plt.ylim([0,1])\n",
        "\n",
        "    plt.legend()"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiekN217JQ05"
      },
      "source": [
        "mpl.rcParams['figure.figsize'] = (20, 18)\n",
        "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
        "\n",
        "def plot_roc(name, labels, predictions, **kwargs):\n",
        "  fp, tp, _ = metrics.roc_curve(labels, predictions)\n",
        "\n",
        "  plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n",
        "  plt.xlabel('False positives [%]')\n",
        "  plt.ylabel('True positives [%]')\n",
        "  #plt.xlim([-0.5,20])\n",
        "  #plt.ylim([80,100.5])\n",
        "  plt.grid(True)\n",
        "  ax = plt.gca()\n",
        "  ax.set_aspect('equal')"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_alAECsLW0x4"
      },
      "source": [
        "def roc():\n",
        "  i = 0\n",
        "  for lbl in TESTS:\n",
        "    plot_roc(f\"Train Baseline - {lbl}\", train_labels[lbl], train_predictions_baseline[lbl], color=colors[i])\n",
        "    plot_roc(f\"Test Baseline - {lbl}\", test_labels[lbl], test_predictions_baseline[lbl], color=colors[i], linestyle='--')\n",
        "    i += 1\n",
        "    plt.legend(loc='lower right')\n",
        "\n",
        "#roc()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXOf7AZcZ1iR"
      },
      "source": [
        "def otherMeasures():\n",
        "  i = 0\n",
        "  for lbl in TESTS:\n",
        "    plot_metrics(baseline_history[lbl], i, lbl)\n",
        "    i += 1\n",
        "\n",
        "# otherMeasures"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9TeXVbbZC1H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf9cf769-d1d5-4cba-8d81-1799fd078c33"
      },
      "source": [
        "print(f\"For train set I have a task1Score of {task1Score(train_labels, train_predictions_baseline)}\") # 0.915 reached with random state 42...\n",
        "print(f\"For test set I have a task1Score of {task1Score(test_labels, test_predictions_baseline)}\") # 0.804 reached with random state 42..."
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For train set I have a task1Score of 0.8971276174163547\n",
            "For test set I have a task1Score of 0.8003866510612457\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZIIh76Sb04f"
      },
      "source": [
        "Now let's do b), i.e. same binary classification again for sepsis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v59Uy3MFgpU5"
      },
      "source": [
        "Maybe here we could do a test for imbalanced data and actually do the initializiation towards that bias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikdEvam3n175"
      },
      "source": [
        "lbl = \"LABEL_Sepsis\"\n",
        "neg, pos = np.bincount(y[lbl])\n",
        "total = neg + pos\n",
        "initial_bias = np.log([pos/neg])\n",
        "model = make_model(output_bias=initial_bias)\n",
        "#model.predict(train_features[:10])\n",
        "initial_weights_sepsis = os.path.join(tempfile.mkdtemp(), 'initial_weights_'+lbl)\n",
        "model.save_weights(initial_weights_sepsis)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8y5VjHqtb3Ks",
        "outputId": "c967fbb7-0db0-45c4-aba7-c1c4d0989a50"
      },
      "source": [
        "model_sepsis = make_model()\n",
        "model_sepsis.load_weights(initial_weights_sepsis)\n",
        "baseline_history_sepsis = model_sepsis.fit(\n",
        "    train_features.drop(columns=[\"pid\"]),\n",
        "    train_labels[lbl],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[early_stopping],\n",
        "    validation_data=(val_features.drop(columns=[\"pid\"]), val_labels[lbl]),\n",
        "    verbose=0)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00026: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InXgM3oLcyfT"
      },
      "source": [
        "def task2Score(actual, submission):\n",
        "  return metrics.roc_auc_score(actual['LABEL_Sepsis'], submission)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgWKzJjqcRjJ",
        "outputId": "f48468be-2fad-41ab-a755-f0b9702c3658"
      },
      "source": [
        "train_predictions_baseline_sepsis = model_sepsis.predict(train_features.drop(columns=[\"pid\"]), batch_size=BATCH_SIZE)\n",
        "test_predictions_baseline_sepsis = model_sepsis.predict(test_features.drop(columns=[\"pid\"]), batch_size=BATCH_SIZE)\n",
        "print(f\"Prediction done for LABEL_Sepsis\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction done for LABEL_Sepsis\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hq4i_HyHoy7x"
      },
      "source": [
        "train_predictions_baseline_sepsis = train_predictions_baseline['LABEL_Sepsis']\n",
        "test_predictions_baseline_sepsis = test_predictions_baseline[\"LABEL_Sepsis\"]"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wouesp56dDvB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d73702c-4467-4b69-d90b-698dec9aae4d"
      },
      "source": [
        "print(f\"For train set I have a task2Score of {task2Score(train_labels, train_predictions_baseline_sepsis)}\") # 0.823 reached with random state 42...\n",
        "print(f\"For test set I have a task2Score of {task2Score(test_labels, test_predictions_baseline_sepsis)}\") # 0.711 reached with random state 42..."
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For train set I have a task2Score of 0.8220087365599014\n",
            "For test set I have a task2Score of 0.7285471905378156\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x0a5bIqe6nO"
      },
      "source": [
        "Let's do now the task c):  \n",
        "Predict a more general evolution of the patient state, i.e. mean value of a vital sign in the remaining stay. This is a regression task.\n",
        "The corresponding columns are VITALS. To evaluate the performance of a given model on this sub-task we use [R2-Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7h1nq34pW3W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7725f75-bf8c-48b8-e881-729cb12416f5"
      },
      "source": [
        "clf = {}\n",
        "for lbl in VITALS:\n",
        "  clf[lbl] = Ridge(alpha=1.0)\n",
        "  clf[lbl].fit(train_features.drop(columns=[\"pid\"]), train_labels[lbl])\n",
        "  print(f\"{lbl} clf fit!\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LABEL_RRate clf fit!\n",
            "LABEL_ABPm clf fit!\n",
            "LABEL_SpO2 clf fit!\n",
            "LABEL_Heartrate clf fit!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SR4wPQ4rL7x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "809976fa-4571-4df0-8b5a-11b1f97650df"
      },
      "source": [
        "train_predicted = {}\n",
        "test_predicted = {}\n",
        "for lbl in VITALS:\n",
        "  train_predicted[lbl] = clf[lbl].predict(train_features.drop(columns=[\"pid\"]))\n",
        "  test_predicted[lbl] = clf[lbl].predict(test_features.drop(columns=[\"pid\"]))\n",
        "  print(f\"Prediction of vital {lbl} done\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction of vital LABEL_RRate done\n",
            "Prediction of vital LABEL_ABPm done\n",
            "Prediction of vital LABEL_SpO2 done\n",
            "Prediction of vital LABEL_Heartrate done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlFVvkoTrtmA"
      },
      "source": [
        "def task3Score(actual, submission):\n",
        "  return np.mean([0.5 + 0.5 * np.maximum(0, metrics.r2_score(actual[entry], submission[entry])) for entry in VITALS])"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cU4I-CPrr14t"
      },
      "source": [
        "#print(f\"For train set I have a task3Score of {task3Score(train_labels, train_predicted)}\")\n",
        "#print(f\"For test set I have a task3Score of {task3Score(test_labels, test_predicted)}\")"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0V95NOqsM_D"
      },
      "source": [
        "The following is modified from score_submission.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjEjBktXswye"
      },
      "source": [
        "def get_score(df_true, df_submission):\n",
        "    df_submission = df_submission.sort_values('pid')\n",
        "    df_true = df_true.sort_values('pid')\n",
        "    task1 = np.mean([metrics.roc_auc_score(df_true[entry], df_submission[entry]) for entry in TESTS])\n",
        "    task2 = metrics.roc_auc_score(df_true['LABEL_Sepsis'], df_submission['LABEL_Sepsis'])\n",
        "    task3 = np.mean([0.5 + 0.5 * np.maximum(0, metrics.r2_score(df_true[entry], df_submission[entry])) for entry in VITALS])\n",
        "    score = np.mean([task1, task2, task3])\n",
        "    print(f\"total score is {score} where the individual scores are {task1}, {task2}, {task3}\")\n",
        "    return score"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBy0gbh-smDP"
      },
      "source": [
        "Let's try to combine all together to see the get_score once:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qe2Mw1jELXCn"
      },
      "source": [
        "train_end = dict(train_predictions_baseline, **train_predicted)\n",
        "test_end = dict(test_predictions_baseline, **test_predicted)\n",
        "train_end[\"LABEL_Sepsis\"] = train_predictions_baseline_sepsis \n",
        "test_end[\"LABEL_Sepsis\"] = test_predictions_baseline_sepsis"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWK1UqY1sqvB"
      },
      "source": [
        "train_res = pd.DataFrame(train_features[\"pid\"].values, columns=[\"pid\"])\n",
        "for lbl in train_end.keys():\n",
        "  train_res[lbl] = pd.DataFrame(train_end[lbl], columns=[lbl])\n",
        "test_res = pd.DataFrame(test_features[\"pid\"].values, columns=[\"pid\"])\n",
        "for lbl in test_end.keys():\n",
        "  test_res[lbl] = pd.DataFrame(test_end[lbl], columns=[lbl])"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdwtJtJlLiBg",
        "outputId": "8a74d444-d9a2-4924-9e28-44a684223659"
      },
      "source": [
        "print(f\"Above was train {get_score(train_labels, train_res)}.\")\n",
        "print(f\"Above was test {get_score(test_labels, test_res)}.\")"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total score is 0.8338999172643687 where the individual scores are 0.8971276174163547, 0.8220087365599014, 0.78256339781685\n",
            "Above was train 0.8338999172643687.\n",
            "total score is 0.7596098298022472 where the individual scores are 0.8003866510612457, 0.7285471905378156, 0.7498956478076804\n",
            "Above was test 0.7596098298022472.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV0jOe5KsUZq"
      },
      "source": [
        "Looks like this might actually work!  \n",
        "Let's now train each model one last time with all data and then do the final score try."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q58BMIbpC8-r",
        "outputId": "a579cab8-9a09-4145-abcb-a934e924a0cd"
      },
      "source": [
        "X, scalar = preprocess(trainFeatPD_raw)\n",
        "test_X, _ = preprocess(testFeatPD, scalar=scalar)\n",
        "y = trainLblPD.sort_values('pid')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len of fromDF 227940/12 = 18995.0 i.e. len%12 == 0\n",
            "len of fromDF 151968/12 = 12664.0 i.e. len%12 == 0\n",
            "Taking already trained scalar!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8OriMC3Dh13"
      },
      "source": [
        "Do TESTS final"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tx8wZofdFUBI",
        "outputId": "671c1170-5e37-4bd4-a319-491fb2509b56"
      },
      "source": [
        "# split data and normalize\n",
        "sorted = trainFeatPD_raw.sort_values(['pid', 'Time'])\n",
        "rows, cols = sorted.shape\n",
        "# bring into shape for train_test_split\n",
        "sameShape = pd.DataFrame(sorted.values.reshape(int(rows/12), 12*37))\n",
        "# do the train_test_validation_split\n",
        "train_data_df, val_data_df = train_test_split(sameShape, test_size=0.2, random_state=random_state)\n",
        "# reverse the shaping to before\n",
        "rows, _ = train_data_df.shape\n",
        "train_data_df = pd.DataFrame(train_data_df.values.reshape(rows*12, cols), columns=sorted.columns)\n",
        "rows, _ = val_data_df.shape\n",
        "val_data_df = pd.DataFrame(val_data_df.values.reshape(rows*12, cols), columns=sorted.columns)\n",
        "# preprocess all of them\n",
        "train_features, _ = preprocess(train_data_df, scalar)\n",
        "val_features, _ = preprocess(val_data_df, scalar)\n",
        "# split labels\n",
        "y = trainLblPD.sort_values('pid')\n",
        "train_labels, val_labels = train_test_split(y, test_size=0.2, random_state=random_state)\n",
        "# sort the labels by pid\n",
        "train_labels = train_labels.sort_values(\"pid\")\n",
        "val_labels = val_labels.sort_values(\"pid\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len of fromDF 182352/12 = 15196.0 i.e. len%12 == 0\n",
            "Taking already trained scalar!\n",
            "len of fromDF 45588/12 = 3799.0 i.e. len%12 == 0\n",
            "Taking already trained scalar!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUfXda-_DjV4"
      },
      "source": [
        "baseline_history = {}\n",
        "models = {}\n",
        "  \n",
        "def doOld():\n",
        "  for lbl in TESTS:\n",
        "    model = make_model()\n",
        "    model.layers[-1].bias.assign([0.0])\n",
        "    baseline_history[lbl] = model.fit(\n",
        "        train_features.drop(columns=[\"pid\"]),\n",
        "        train_labels[lbl],\n",
        "        batch_size=BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=[early_stopping],\n",
        "        validation_data=(val_features.drop(columns=[\"pid\"]), val_labels[lbl]),\n",
        "        verbose=0)\n",
        "    models[lbl] = model\n",
        "    print(f\"Model for {lbl} done!\")\n",
        "    \n",
        "# doOld()"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uECA6CnF7H7"
      },
      "source": [
        "def doOldPredict():\n",
        "  tests_predicted = {}\n",
        "  for lbl in TESTS:\n",
        "    tests_predicted[lbl] = models[lbl].predict(test_X.drop(columns=[\"pid\"]), batch_size=BATCH_SIZE)\n",
        "    print(f\"Prediction done for {lbl}\")\n",
        "\n",
        "# doOldPredict()"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDrQBxevGxnj"
      },
      "source": [
        "Do Sepsis final"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3Hk3ev8Gm3l"
      },
      "source": [
        "# This makes an initial bias\n",
        "lbl = \"LABEL_Sepsis\"\n",
        "neg, pos = np.bincount(y[lbl])\n",
        "total = neg + pos\n",
        "initial_bias = np.log([pos/neg])\n",
        "model = make_model(output_bias=initial_bias)\n",
        "#model.predict(train_features[:10])\n",
        "initial_weights_sepsis = os.path.join(tempfile.mkdtemp(), 'initial_weights_'+lbl)\n",
        "model.save_weights(initial_weights_sepsis)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odlb1Mt0G3Rk"
      },
      "source": [
        "def doOldSepsis():\n",
        "  model_sepsis = make_model()\n",
        "  model_sepsis.load_weights(initial_weights_sepsis)\n",
        "  baseline_history_sepsis = model_sepsis.fit(\n",
        "      train_features.drop(columns=[\"pid\"]),\n",
        "      train_labels[lbl],\n",
        "      batch_size=BATCH_SIZE,\n",
        "      epochs=EPOCHS,\n",
        "      callbacks=[early_stopping],\n",
        "      validation_data=(val_features.drop(columns=[\"pid\"]), val_labels[lbl]),\n",
        "      verbose=0)\n",
        "  return model_sepsis\n",
        "\n",
        "#model_sepsis = doOldSepsis()"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm9GTt0-HGr5"
      },
      "source": [
        "def predictOldSepsis():\n",
        "  sepsis_predicted = model_sepsis.predict(test_X.drop(columns=[\"pid\"]), batch_size=BATCH_SIZE)\n",
        "  print(f\"Prediction done for LABEL_Sepsis\")\n",
        "  return sepsis_predicted\n",
        "\n",
        "#sepsis_predicted = predictOldSepsis()"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mr9Sql5OswlN"
      },
      "source": [
        "Try one go with one model for TESTS+sepsis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9fZzE6qsvDY",
        "outputId": "b5f3a675-5a5b-4891-de3c-5daa942c7d1a"
      },
      "source": [
        "model = allTESTS(train_features, train_labels, val_features, val_labels)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00023: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "qUxlF13VtEGI",
        "outputId": "3b2c0f9a-1135-465c-8d0d-89beef8d664d"
      },
      "source": [
        "tests_predicted = doAllTests(model, test_X)\n",
        "tests_predicted"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>LABEL_BaseExcess</th>\n",
              "      <th>LABEL_Fibrinogen</th>\n",
              "      <th>LABEL_AST</th>\n",
              "      <th>LABEL_Alkalinephos</th>\n",
              "      <th>LABEL_Bilirubin_total</th>\n",
              "      <th>LABEL_Lactate</th>\n",
              "      <th>LABEL_TroponinI</th>\n",
              "      <th>LABEL_SaO2</th>\n",
              "      <th>LABEL_Bilirubin_direct</th>\n",
              "      <th>LABEL_EtCO2</th>\n",
              "      <th>LABEL_Sepsis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.782933</td>\n",
              "      <td>0.672191</td>\n",
              "      <td>0.978571</td>\n",
              "      <td>0.987618</td>\n",
              "      <td>0.985059</td>\n",
              "      <td>0.661743</td>\n",
              "      <td>0.013152</td>\n",
              "      <td>0.430337</td>\n",
              "      <td>0.401781</td>\n",
              "      <td>0.021800</td>\n",
              "      <td>0.120071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.014278</td>\n",
              "      <td>0.011855</td>\n",
              "      <td>0.190448</td>\n",
              "      <td>0.193228</td>\n",
              "      <td>0.205053</td>\n",
              "      <td>0.036682</td>\n",
              "      <td>0.109982</td>\n",
              "      <td>0.034584</td>\n",
              "      <td>0.010962</td>\n",
              "      <td>0.005575</td>\n",
              "      <td>0.024984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.060708</td>\n",
              "      <td>0.039840</td>\n",
              "      <td>0.213591</td>\n",
              "      <td>0.216848</td>\n",
              "      <td>0.208401</td>\n",
              "      <td>0.073829</td>\n",
              "      <td>0.101047</td>\n",
              "      <td>0.102489</td>\n",
              "      <td>0.024874</td>\n",
              "      <td>0.013908</td>\n",
              "      <td>0.025436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.473054</td>\n",
              "      <td>0.848914</td>\n",
              "      <td>0.981806</td>\n",
              "      <td>0.985494</td>\n",
              "      <td>0.985189</td>\n",
              "      <td>0.752709</td>\n",
              "      <td>0.032107</td>\n",
              "      <td>0.605220</td>\n",
              "      <td>0.482894</td>\n",
              "      <td>0.065916</td>\n",
              "      <td>0.087024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.180231</td>\n",
              "      <td>0.053243</td>\n",
              "      <td>0.150614</td>\n",
              "      <td>0.152541</td>\n",
              "      <td>0.144320</td>\n",
              "      <td>0.108486</td>\n",
              "      <td>0.022076</td>\n",
              "      <td>0.090892</td>\n",
              "      <td>0.014362</td>\n",
              "      <td>0.004558</td>\n",
              "      <td>0.034053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12659</th>\n",
              "      <td>0.037563</td>\n",
              "      <td>0.029785</td>\n",
              "      <td>0.221136</td>\n",
              "      <td>0.197628</td>\n",
              "      <td>0.210160</td>\n",
              "      <td>0.032462</td>\n",
              "      <td>0.012869</td>\n",
              "      <td>0.019388</td>\n",
              "      <td>0.009874</td>\n",
              "      <td>0.001925</td>\n",
              "      <td>0.018005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12660</th>\n",
              "      <td>0.801476</td>\n",
              "      <td>0.143467</td>\n",
              "      <td>0.352687</td>\n",
              "      <td>0.320399</td>\n",
              "      <td>0.293583</td>\n",
              "      <td>0.615172</td>\n",
              "      <td>0.057113</td>\n",
              "      <td>0.462550</td>\n",
              "      <td>0.050652</td>\n",
              "      <td>0.053806</td>\n",
              "      <td>0.088313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12661</th>\n",
              "      <td>0.844711</td>\n",
              "      <td>0.011014</td>\n",
              "      <td>0.016583</td>\n",
              "      <td>0.027093</td>\n",
              "      <td>0.019256</td>\n",
              "      <td>0.242190</td>\n",
              "      <td>0.002726</td>\n",
              "      <td>0.559636</td>\n",
              "      <td>0.001666</td>\n",
              "      <td>0.000469</td>\n",
              "      <td>0.029351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12662</th>\n",
              "      <td>0.007349</td>\n",
              "      <td>0.020716</td>\n",
              "      <td>0.367023</td>\n",
              "      <td>0.354466</td>\n",
              "      <td>0.395569</td>\n",
              "      <td>0.052729</td>\n",
              "      <td>0.215693</td>\n",
              "      <td>0.034549</td>\n",
              "      <td>0.024417</td>\n",
              "      <td>0.047700</td>\n",
              "      <td>0.043697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12663</th>\n",
              "      <td>0.030668</td>\n",
              "      <td>0.072447</td>\n",
              "      <td>0.383451</td>\n",
              "      <td>0.395639</td>\n",
              "      <td>0.405441</td>\n",
              "      <td>0.073711</td>\n",
              "      <td>0.272497</td>\n",
              "      <td>0.074983</td>\n",
              "      <td>0.047774</td>\n",
              "      <td>0.060899</td>\n",
              "      <td>0.024055</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12664 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       LABEL_BaseExcess  LABEL_Fibrinogen  ...  LABEL_EtCO2  LABEL_Sepsis\n",
              "0              0.782933          0.672191  ...     0.021800      0.120071\n",
              "1              0.014278          0.011855  ...     0.005575      0.024984\n",
              "2              0.060708          0.039840  ...     0.013908      0.025436\n",
              "3              0.473054          0.848914  ...     0.065916      0.087024\n",
              "4              0.180231          0.053243  ...     0.004558      0.034053\n",
              "...                 ...               ...  ...          ...           ...\n",
              "12659          0.037563          0.029785  ...     0.001925      0.018005\n",
              "12660          0.801476          0.143467  ...     0.053806      0.088313\n",
              "12661          0.844711          0.011014  ...     0.000469      0.029351\n",
              "12662          0.007349          0.020716  ...     0.047700      0.043697\n",
              "12663          0.030668          0.072447  ...     0.060899      0.024055\n",
              "\n",
              "[12664 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCMzMuD3Ecb3"
      },
      "source": [
        "Do VITALS final"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4KBKdnwEc0Q",
        "outputId": "b611d030-5b71-4e23-c4e0-55bbba3afd74"
      },
      "source": [
        "clf = {}\n",
        "for lbl in VITALS:\n",
        "  clf[lbl] = Ridge(alpha=1.0)\n",
        "  clf[lbl].fit(X.drop(columns=[\"pid\"]), y[lbl])\n",
        "  print(f\"{lbl} clf fit!\")"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LABEL_RRate clf fit!\n",
            "LABEL_ABPm clf fit!\n",
            "LABEL_SpO2 clf fit!\n",
            "LABEL_Heartrate clf fit!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxBEnjBeEna4",
        "outputId": "8ffb3474-643c-4be3-936d-d3262895833b"
      },
      "source": [
        "vitals_predicted = {}\n",
        "for lbl in VITALS:\n",
        "  vitals_predicted[lbl] = clf[lbl].predict(test_X.drop(columns=[\"pid\"]))\n",
        "  print(f\"Prediction of vital {lbl} done\")"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction of vital LABEL_RRate done\n",
            "Prediction of vital LABEL_ABPm done\n",
            "Prediction of vital LABEL_SpO2 done\n",
            "Prediction of vital LABEL_Heartrate done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xz0DKNrxHW_W"
      },
      "source": [
        "Combine them all"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIeL_qEBHYx4"
      },
      "source": [
        "final_end = dict(tests_predicted, **vitals_predicted)\n",
        "#final_end[\"LABEL_Sepsis\"] = sepsis_predicted"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSdUA-FwHqQU"
      },
      "source": [
        "res = pd.DataFrame(test_X[\"pid\"].values, columns=[\"pid\"])\n",
        "for lbl in final_end.keys():\n",
        "  res[lbl] = pd.DataFrame(final_end[lbl], columns=[lbl])\n",
        "#res"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJjtPn8YstIy"
      },
      "source": [
        "This is to output the final zip to be downloaded"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvyuMd0XbWu6"
      },
      "source": [
        "filename = \"finalOut.zip\"\n",
        "res.to_csv(filename, index=False, float_format='%.3f', compression='zip')"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-ySncgcusD_"
      },
      "source": [
        "Run the score_submission to test if it works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0PHsrZ3uoEM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a17bc438-72be-4203-bac9-e4b48fb84fdc"
      },
      "source": [
        "# filename = 'sample.csv'\n",
        "df_submission = pd.read_csv(filename)\n",
        "\n",
        "# generate a baseline based on sample.zip\n",
        "df_true = pd.read_csv(filename)\n",
        "for label in TESTS + ['LABEL_Sepsis']:\n",
        "    # round classification labels\n",
        "    df_true[label] = np.around(df_true[label].values)\n",
        "\n",
        "get_score(df_true, df_submission)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total score is 1.0 where the individual scores are 1.0, 1.0, 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    }
  ]
}