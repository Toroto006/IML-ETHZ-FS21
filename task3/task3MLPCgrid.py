# -*- coding: utf-8 -*-
"""Task3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/Toroto006/iml2021/blob/main/task3/Task3.ipynb

### Trying to solve Task 3 now!
"""

import pandas as pd
import math
import operator
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import f1_score
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import OneHotEncoder
from sklearn.neural_network import MLPClassifier
from itertools import product
import numpy as np

trainUrl = 'https://raw.githubusercontent.com/Toroto006/iml2021/main/task3/train.csv?token=AH44KNUX4PZX2ZQVSNK7UFLATNZ64'
testUrl = 'https://raw.githubusercontent.com/Toroto006/iml2021/main/task3/test.csv?token=AH44KNVTLV33VIFDUBUKCTTATNZ6W'

trainPD = pd.read_csv(trainUrl)
trainPD.head(3)

testPD = pd.read_csv(testUrl)
testPD.head(3)

"""Let's first take a more thorough look at the data."""

aminoAcids = {
    "R" : 1,
    "H" : 2,
    "K" : 3,
    "D" : 4,
    "E" : 5,
    "S" : 6,
    "T" : 7,
    "N" : 8,
    "Q" : 9,
    "C" : 10,
    "U" : 11,
    "G" : 12,
    "P" : 13,
    "A" : 14,
    "I" : 15,
    "L" : 16,
    "M" : 17,
    "F" : 18,
    "W" : 19,
    "Y" : 20,
    "V" : 21,
}

def really_safe_normalise_in_place(d):
    factor=1.0/math.fsum(d.values())
    for k in d:
        d[k] = d[k]*factor
    key_for_max = max(d.items(), key=operator.itemgetter(1))[0]
    diff = 1.0 - math.fsum(d.values())
    #print "discrepancy = " + str(diff)
    d[key_for_max] += diff
    return d

#aminoAcids = really_safe_normalise_in_place(aminoAcids)
aminoAcids["D"]

splitNames = ["fir", "sec", "thr", "for"]

def splitPlaces(dfs, enc=None):
  #splits = dfs.apply(lambda df: [aminoAcids.get(item,item) for item in list(df["Sequence"])], axis=1, result_type='expand').rename(columns={0:"fir", 1:"sec", 2:"thr", 3:"for"})
  splits = dfs.apply(lambda df: list(df["Sequence"]), axis=1, result_type='expand').rename(columns={0:"fir", 1:"sec", 2:"thr", 3:"for"})
  #print(splits.shape)
  if enc is None:
    enc = OneHotEncoder()
  enc.fit(splits)
  onehotlabels = enc.transform(splits).toarray()
  #print(onehotlabels.shape)
  new_dfs = dfs.copy()
  #print(enc.get_feature_names())
  new_dfs[enc.get_feature_names()] = onehotlabels
  return new_dfs, enc

splitPD, _ = splitPlaces(trainPD)
splitPD.head(5)

"""To try the idea do train and test split."""

random_state = 42
# split labels
Xs = splitPD.drop("Sequence", axis='columns')
train_split, test_split = train_test_split(Xs, test_size=0.2, random_state=random_state)
train_y = train_split["Active"]
train_X = train_split.drop("Active", axis='columns')
test_y = test_split["Active"]
test_X = test_split.drop("Active", axis='columns')
print(train_X.head(5))
print(train_y.head(5))

"""Lets try a SVM:"""

def trySVC():
  svcInit = SVC(class_weight='balanced', random_state=random_state)
  svc = svcInit.fit(train_X, train_y)
  train_predicted = svc.predict(train_X)
  test_predicted = svc.predict(test_X)
  print(f"train score: {f1_score(train_y, train_predicted)}")
  print(f"test score: {f1_score(test_y, test_predicted)}")

# trySVC()

"""Looks quite good, lets do a gridsearch."""

def findBest():
  parameters = {'kernel':('linear', 'poly', 'rbf'), 'C':[1, 10]}
  clf = GridSearchCV(svcInit, parameters)
  clf = clf.fit(train_X, train_y)
  train_predicted = clf.predict(train_X)
  test_predicted = clf.predict(test_X)
  print(f"train score: {f1_score(train_y, train_predicted)}") # 0.99229
  print(f"test score: {f1_score(test_y, test_predicted)}") # 0.88872
  print(clf.get_params()['estimator'])
  # SVC(C=1.0, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,
  # decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',
  # max_iter=-1, probability=False, random_state=42, shrinking=True, tol=0.001,
  # verbose=False)

"""Wow, looks good!"""

X, enc = splitPlaces(trainPD)
y = X["Active"]
X = X.drop(["Sequence", "Active"], axis='columns')

testedClf = SVC(C=1.0, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,
     decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',
     max_iter=-1, probability=False, random_state=42, shrinking=True, tol=0.001,
     verbose=False)
#testedClf = testedClf.fit(X, y)

"""Prepare submission data:"""

test_Xs, _ = splitPlaces(testPD, enc)
testXs = test_Xs.drop("Sequence", axis='columns')

#submission_predicted = testedClf.predict(testXs)
#submission_predicted = pd.DataFrame(submission_predicted, columns=["Active"])

"""Lets try an MLPClassifier again"""

#clfMLP = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(100, ), random_state=random_state)
#clfMLP.fit(train_X, train_y)

#train_predicted = clfMLP.predict(train_X)
#test_predicted = clfMLP.predict(test_X)
#print(f"train score: {f1_score(train_y, train_predicted)}")
#print(f"test score: {f1_score(test_y, test_predicted)}")

"""Look like it's overfitting a lot, lets try a grid search"""

parameters={
'learning_rate': ["constant", "invscaling", "adaptive"],
'hidden_layer_sizes': [x for x in product(range(10,100,5), range(1,10,2))],
'alpha': 10.0 ** -np.arange(1, 7),
'activation': ["logistic", "relu", "Tanh"]
}
clfMLPCgrid = GridSearchCV(estimator=MLPClassifier(solver='lbfgs', random_state=random_state),param_grid=parameters,n_jobs=-1,verbose=2,cv=10)
clfMLPCgrid.fit(X, y)

train_predicted = clfMLPCgrid.predict(train_X)
test_predicted = clfMLPCgrid.predict(test_X)
print(f"train score: {f1_score(train_y, train_predicted)}")
print(f"test score: {f1_score(test_y, test_predicted)}")

submission_predicted = clfMLPCgrid.predict(testXs)
submission_predicted = pd.DataFrame(submission_predicted, columns=["Active"])

submission_predicted.to_csv("testOut.csv", index=False, header=False)
